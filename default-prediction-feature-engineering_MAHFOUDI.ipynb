{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":35332,"databundleVersionId":3723648,"sourceType":"competition"},{"sourceId":3711380,"sourceType":"datasetVersion","datasetId":2219908},{"sourceId":3714104,"sourceType":"datasetVersion","datasetId":2221541}],"dockerImageVersionId":30198,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Feature Engineering and Preprocessing Overview\n\nThis notebook combines feature engineering and data preprocessing for customer datasets, designed to optimize memory usage and prepare the data for deep learning. Leveraging **RAPIDS cuDF** and **cuPy**, it processes data in chunks and organizes it for efficient handling in memory-constrained environments.\n\n### **References**\nFor more discussions on feature engineering and preprocessing:\n- [Kaggle Discussion 1](https://www.kaggle.com/competitions/amex-default-prediction/discussion/327828)\n- [Kaggle Discussion 2](https://www.kaggle.com/competitions/amex-default-prediction/discussion/328054)\n\nDiscussions about data preprocessing are [here][1] and [here][2]\n\n[Kaggle Discussion 3](https://www.kaggle.com/competitions/amex-default-prediction/discussion/327828)\n[Kaggle Discussion 4](https://www.kaggle.com/competitions/amex-default-prediction/discussion/328054)\n\nCheck the notebook where we got the ideas:\n[kaggle Notebook](https://www.kaggle.com/code/cdeotte/tensorflow-gru-starter-0-790)","metadata":{"_uuid":"5335c733-3ae1-45d9-a7e7-5df0064979e0","_cell_guid":"a7827b8d-f065-46fb-9a84-6657cc7f559d","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"PROC_DATA = True","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-05T22:41:33.480356Z","iopub.execute_input":"2024-12-05T22:41:33.480727Z","iopub.status.idle":"2024-12-05T22:41:33.484660Z","shell.execute_reply.started":"2024-12-05T22:41:33.480701Z","shell.execute_reply":"2024-12-05T22:41:33.483858Z"}},"outputs":[],"execution_count":6},{"cell_type":"markdown","source":"# PreProcessing and Chunking","metadata":{"_uuid":"066b7b32-4943-40a3-8b30-7285c468e877","_cell_guid":"75b37427-48b5-4290-9eaf-ffde64f7410a","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"import os\nimport gc\nimport cudf\nimport pandas as pd\nimport numpy as np\nimport cupy\n\n\ndef get_row_boundaries(total_rows: int, num_files: int, verbose: str = '') -> list:\n    if num_files <= 0:\n        raise ValueError(\"num_files must be a positive integer.\")\n    \n    chunk_size = total_rows // num_files\n    boundaries = [(k * chunk_size, (k + 1) * chunk_size - 1 if k != num_files - 1 else total_rows - 1) \n                  for k in range(num_files)]\n    \n    if verbose:\n        print(f'Splitting {verbose} data into {num_files} files:')\n        for i, (start, end) in enumerate(boundaries, 1):\n            print(f'  Chunk {i}: {start} to {end}')\n\n    return boundaries\n\ndef save_boundaries_to_npy(train_boundaries, test_boundaries, output_dir):\n    os.makedirs(output_dir, exist_ok=True)\n    np.save(f\"{output_dir}/train_boundaries.npy\", train_boundaries)\n    np.save(f\"{output_dir}/test_boundaries.npy\", test_boundaries)\n    print(f\"Boundaries saved to {output_dir} as .npy files.\")\n\ndef load_boundaries_from_npy(output_dir):\n    train_boundaries = np.load(f\"{output_dir}/train_boundaries.npy\", allow_pickle=True)\n    test_boundaries = np.load(f\"{output_dir}/test_boundaries.npy\", allow_pickle=True)\n    print(f\"Boundaries loaded from {output_dir}.\")\n    return train_boundaries, test_boundaries\n\ndef process_train_test_data(process_data: bool, \n                            path_to_customer_hashes: str = None, \n                            train_num_files: int = 10,\n                            test_num_files: int = 5,\n                            output_dir: str = './boundaries'):\n    train_file_path = f\"{output_dir}/train_boundaries.npy\"\n    test_file_path = f\"{output_dir}/test_boundaries.npy\"\n\n    if os.path.exists(train_file_path) and os.path.exists(test_file_path):\n        print(\"Boundary files already exist. Loading boundaries.\")\n        return load_boundaries_from_npy(output_dir)\n    else:\n        print(\"Boundary files not found. Processing data to generate boundaries.\")\n\n        targets = cudf.read_csv('../input/amex-default-prediction/train_labels.csv')\n        targets['customer_ID'] = (targets['customer_ID'].str[-16:]\n                                  .str.hex_to_int()\n                                  .astype('int64'))\n        print(f'Train targets loaded: {targets.shape[0]} rows')\n        del targets\n        gc.collect()\n\n        if path_to_customer_hashes:\n            train = cudf.read_parquet(f'{path_to_customer_hashes}train_customer_hashes.pqt').to_pandas()\n        else:\n            train = pd.read_csv('/kaggle/input/amex-default-prediction/train_data.csv', \n                                usecols=['customer_ID'], dtype=str, memory_map=True)\n        \n        print(f'Train data loaded: {train.shape[0]} rows')\n        train_boundaries = get_row_boundaries(train.shape[0], num_files=train_num_files, verbose='train')\n        del train\n        gc.collect()\n\n        test = pd.read_csv('/kaggle/input/amex-default-prediction/test_data.csv', \n                           usecols=['customer_ID'], dtype=str, memory_map=True)\n        \n        print(f'Test data loaded: {test.shape[0]} rows')\n        test_boundaries = get_row_boundaries(test.shape[0], num_files=test_num_files, verbose='test')\n        del test\n        gc.collect()\n\n        save_boundaries_to_npy(train_boundaries, test_boundaries, output_dir)\n        return train_boundaries, test_boundaries\n\nPROCESS_DATA = True\nPATH_TO_CUSTOMER_HASHES = None\nTRAIN_NUM_FILES = 10\nTEST_NUM_FILES = 20\nOUTPUT_DIR = './boundaries'\n\ntrain_boundaries, test_boundaries = process_train_test_data(\n    process_data=PROCESS_DATA, \n    path_to_customer_hashes=PATH_TO_CUSTOMER_HASHES,\n    train_num_files=TRAIN_NUM_FILES,\n    test_num_files=TEST_NUM_FILES,\n    output_dir=OUTPUT_DIR\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-05T22:25:18.864991Z","iopub.execute_input":"2024-12-05T22:25:18.865481Z","iopub.status.idle":"2024-12-05T22:40:36.962651Z","shell.execute_reply.started":"2024-12-05T22:25:18.865442Z","shell.execute_reply":"2024-12-05T22:40:36.961733Z"}},"outputs":[{"name":"stdout","text":"Boundary files not found. Processing data to generate boundaries.\nTrain targets loaded: 458913 rows\nTrain data loaded: 5531451 rows\nSplitting train data into 10 files:\n  Chunk 1: 0 to 553144\n  Chunk 2: 553145 to 1106289\n  Chunk 3: 1106290 to 1659434\n  Chunk 4: 1659435 to 2212579\n  Chunk 5: 2212580 to 2765724\n  Chunk 6: 2765725 to 3318869\n  Chunk 7: 3318870 to 3872014\n  Chunk 8: 3872015 to 4425159\n  Chunk 9: 4425160 to 4978304\n  Chunk 10: 4978305 to 5531450\nTest data loaded: 11363762 rows\nSplitting test data into 20 files:\n  Chunk 1: 0 to 568187\n  Chunk 2: 568188 to 1136375\n  Chunk 3: 1136376 to 1704563\n  Chunk 4: 1704564 to 2272751\n  Chunk 5: 2272752 to 2840939\n  Chunk 6: 2840940 to 3409127\n  Chunk 7: 3409128 to 3977315\n  Chunk 8: 3977316 to 4545503\n  Chunk 9: 4545504 to 5113691\n  Chunk 10: 5113692 to 5681879\n  Chunk 11: 5681880 to 6250067\n  Chunk 12: 6250068 to 6818255\n  Chunk 13: 6818256 to 7386443\n  Chunk 14: 7386444 to 7954631\n  Chunk 15: 7954632 to 8522819\n  Chunk 16: 8522820 to 9091007\n  Chunk 17: 9091008 to 9659195\n  Chunk 18: 9659196 to 10227383\n  Chunk 19: 10227384 to 10795571\n  Chunk 20: 10795572 to 11363761\nBoundaries saved to ./boundaries as .npy files.\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"print(train_boundaries) \n\nprint(test_boundaries)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-05T22:41:21.306049Z","iopub.execute_input":"2024-12-05T22:41:21.306831Z","iopub.status.idle":"2024-12-05T22:41:21.310700Z","shell.execute_reply.started":"2024-12-05T22:41:21.306797Z","shell.execute_reply":"2024-12-05T22:41:21.309938Z"}},"outputs":[{"name":"stdout","text":"[(0, 553144), (553145, 1106289), (1106290, 1659434), (1659435, 2212579), (2212580, 2765724), (2765725, 3318869), (3318870, 3872014), (3872015, 4425159), (4425160, 4978304), (4978305, 5531450)]\n[(0, 568187), (568188, 1136375), (1136376, 1704563), (1704564, 2272751), (2272752, 2840939), (2840940, 3409127), (3409128, 3977315), (3977316, 4545503), (4545504, 5113691), (5113692, 5681879), (5681880, 6250067), (6250068, 6818255), (6818256, 7386443), (7386444, 7954631), (7954632, 8522819), (8522820, 9091007), (9091008, 9659195), (9659196, 10227383), (10227384, 10795571), (10795572, 11363761)]\n","output_type":"stream"}],"execution_count":4},{"cell_type":"markdown","source":"# Feature Engineering \n","metadata":{"_uuid":"f813e067-5840-4ba5-87d2-d153c0299109","_cell_guid":"083356ce-3d04-4f54-b8c4-a991b4fb3138","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"\n\nD_63_MAP = {'CL': 2, 'CO': 3, 'CR': 4, 'XL': 5, 'XM': 6, 'XZ': 7}\nD_64_MAP = {'-1': 2, 'O': 3, 'R': 4, 'U': 5}\n\nCAT_OFFSETS = {\n    'B_30': 2, 'B_38': 1, 'D_114': 2, 'D_116': 2, 'D_117': 3,\n    'D_120': 2, 'D_126': 3, 'D_66': 2,  'D_68': 2\n}\nCATS_ADDITIONAL = ['D_63', 'D_64']\n\nDTYPE_SKIP = {'customer_ID', 'year', 'month', 'day'}\n\ndef _extract_date_parts(df, date_col='S_2'):\n    df[date_col] = cudf.to_datetime(df[date_col])\n    df['year'] = (df[date_col].dt.year - 2000).astype('int8')\n    df['month'] = df[date_col].dt.month.astype('int8')\n    df['day'] = df[date_col].dt.day.astype('int8')\n    del df[date_col]\n\ndef _encode_categoricals(df):\n    df['D_63'] = df['D_63'].map(D_63_MAP).fillna(1).astype('int8')\n    df['D_64'] = df['D_64'].map(D_64_MAP).fillna(1).astype('int8')\n\n    for c, offset in CAT_OFFSETS.items():\n        df[c] = (df[c] + offset).fillna(1).astype('int8')\n\n    return list(CAT_OFFSETS.keys()) + CATS_ADDITIONAL\n\ndef _downcast_dtypes(df):\n    for c in df.columns:\n        if c in DTYPE_SKIP:\n            continue\n        dtype_str = str(df[c].dtype)\n        if dtype_str == 'int64':\n            df[c] = df[c].astype('int32')\n        elif dtype_str == 'float64':\n            df[c] = df[c].astype('float32')\n\ndef _pad_rows(df, cats, desired_length=13):\n    counts = df.groupby('customer_ID').size()\n    if (counts < desired_length).any():\n        needed_rows_all = []\n        for j in range(1, desired_length):\n            idx = counts.index[counts == j]\n            if len(idx) > 0:\n                needed = cupy.repeat(idx.values, desired_length - j)\n                needed_rows_all.append(needed)\n\n        if needed_rows_all:\n            needed_rows_all = cupy.concatenate(needed_rows_all)\n\n            pad_df = df.head(len(needed_rows_all)).copy()\n            pad_df = pad_df * 0 - 1\n            pad_df[cats] = 0\n            pad_df['customer_ID'] = needed_rows_all\n            df = cudf.concat([df, pad_df], axis=0, ignore_index=True)\n\n    return df\n\ndef _final_column_ordering(df, cats):\n    cols = list(df.columns)\n    cols.remove('customer_ID')\n    non_cats = [c for c in cols if c not in cats]\n    final_cols = ['customer_ID'] + cats + non_cats\n    return df[final_cols]\n\ndef feature_engineer(df, PAD_CUSTOMER_TO_13_ROWS=True, targets=None):\n    df['customer_ID'] = df['customer_ID'].str[-16:].str.hex_to_int().astype('int64')\n\n    _extract_date_parts(df)\n\n    cats = _encode_categoricals(df)\n\n    _downcast_dtypes(df)\n\n    if PAD_CUSTOMER_TO_13_ROWS:\n        df = _pad_rows(df, cats)\n\n    if targets is not None:\n        df = df.merge(targets, on='customer_ID', how='left')\n        df['target'] = df['target'].astype('int8')\n\n    df = df.fillna(-0.5)\n\n    df = df.sort_values(['customer_ID', 'year', 'month', 'day']).reset_index(drop=True)\n    df = df.drop(['year', 'month', 'day'], axis=1)\n\n    df = _final_column_ordering(df, cats)\n\n    return df\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-05T22:41:26.938619Z","iopub.execute_input":"2024-12-05T22:41:26.939439Z","iopub.status.idle":"2024-12-05T22:41:26.955482Z","shell.execute_reply.started":"2024-12-05T22:41:26.939401Z","shell.execute_reply":"2024-12-05T22:41:26.954722Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"TRAIN_DATA_PATH = '../input/amex-default-prediction/train_data.csv'\nTEST_DATA_PATH = '../input/amex-default-prediction/test_data.csv'\nTRAIN_LABELS_PATH = '../input/amex-default-prediction/train_labels.csv'\nOUTPUT_PATH = './processed_data'\n\nos.makedirs(OUTPUT_PATH, exist_ok=True)\n\ndef load_column_names(csv_path):\n    temp_df = cudf.read_csv(csv_path, nrows=1)\n    return temp_df.columns\n\ndef load_targets(csv_path):\n    targets = cudf.read_csv(csv_path)\n    targets['customer_ID'] = targets['customer_ID'].str[-16:].str.hex_to_int().astype('int64')\n    return targets\n\ndef process_chunk(csv_path, boundaries, column_names, is_train=True, targets=None, output_prefix='data'):\n    for k, (start_idx, end_idx) in enumerate(boundaries, start=1):\n        num_rows = end_idx - start_idx + 1\n\n        df = cudf.read_csv(\n            csv_path,\n            nrows=num_rows,\n            skiprows=start_idx + 1,\n            header=None,\n            names=column_names\n        )\n\n        df = feature_engineer(df, targets=targets if is_train else None)\n\n        unique_customers = df['customer_ID'].nunique()\n        print(f\"{'Train' if is_train else 'Test'}_File_{k} has {unique_customers} customers and shape {df.shape}\")\n\n        if is_train:\n            tar = df[['customer_ID', 'target']].drop_duplicates().sort_index()\n            tar_output_path = os.path.join(OUTPUT_PATH, f'targets_{k}.pqt')\n            tar.to_parquet(tar_output_path, index=False)\n\n        if is_train:\n            arr = df.iloc[:, 1:-1].values.reshape((-1, 13, 188)).astype('float32')\n        else:\n            arr = df.iloc[:, 1:].values.reshape((-1, 13, 188)).astype('float32')\n\n        data_output_path = os.path.join(OUTPUT_PATH, f'{output_prefix}_{k}')\n        cupy.save(data_output_path, arr)\n\n        del df, arr\n        if is_train:\n            del tar\n        gc.collect()\n\ndef process_all(train_boundaries, test_boundaries, process_traindata=True, process_testdata=True):\n    column_names = load_column_names(TRAIN_DATA_PATH)\n    targets = load_targets(TRAIN_LABELS_PATH)\n\n    if process_traindata:\n        process_chunk(\n            csv_path=TRAIN_DATA_PATH,\n            boundaries=train_boundaries,\n            column_names=column_names,\n            is_train=True,\n            targets=targets,\n            output_prefix='data'\n        )\n        del targets\n        gc.collect()\n\n    if process_testdata:\n        process_chunk(\n            csv_path=TEST_DATA_PATH,\n            boundaries=test_boundaries,\n            column_names=column_names,\n            is_train=False,\n            targets=None,\n            output_prefix='test_data'\n        )\n\nif PROC_DATA:\n    process_all(train_boundaries, test_boundaries, process_traindata=True, process_testdata=True)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-05T22:41:40.558041Z","iopub.execute_input":"2024-12-05T22:41:40.558513Z"}},"outputs":[{"name":"stdout","text":"Train_File_1 has 45869 customers and shape (596297, 190)\nTrain_File_2 has 45918 customers and shape (596934, 190)\nTrain_File_3 has 45815 customers and shape (595595, 190)\nTrain_File_4 has 45796 customers and shape (595348, 190)\nTrain_File_5 has 45985 customers and shape (597805, 190)\nTrain_File_6 has 45957 customers and shape (597441, 190)\nTrain_File_7 has 45918 customers and shape (596934, 190)\nTrain_File_8 has 45892 customers and shape (596596, 190)\nTrain_File_9 has 45863 customers and shape (596219, 190)\nTrain_File_10 has 45907 customers and shape (596791, 190)\nTest_File_1 has 46251 customers and shape (601263, 189)\nTest_File_2 has 46209 customers and shape (600717, 189)\nTest_File_3 has 46136 customers and shape (599768, 189)\nTest_File_4 has 46257 customers and shape (601341, 189)\nTest_File_5 has 46284 customers and shape (601692, 189)\nTest_File_6 has 46244 customers and shape (601172, 189)\nTest_File_7 has 46236 customers and shape (601068, 189)\nTest_File_8 has 46281 customers and shape (601653, 189)\nTest_File_9 has 46200 customers and shape (600600, 189)\nTest_File_10 has 46288 customers and shape (601744, 189)\nTest_File_11 has 46211 customers and shape (600743, 189)\nTest_File_12 has 46187 customers and shape (600431, 189)\nTest_File_13 has 46224 customers and shape (600912, 189)\nTest_File_14 has 46221 customers and shape (600873, 189)\nTest_File_15 has 46220 customers and shape (600860, 189)\nTest_File_16 has 46175 customers and shape (600275, 189)\nTest_File_17 has 46219 customers and shape (600847, 189)\nTest_File_18 has 46248 customers and shape (601224, 189)\nTest_File_19 has 46300 customers and shape (601900, 189)\nTest_File_20 has 46247 customers and shape (601211, 189)\n","output_type":"stream"}],"execution_count":null}]}