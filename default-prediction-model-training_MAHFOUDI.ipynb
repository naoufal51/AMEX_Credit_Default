{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":35332,"databundleVersionId":3723648,"sourceType":"competition"},{"sourceId":211460454,"sourceType":"kernelVersion"}],"dockerImageVersionId":30805,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# PyTorch Transformer\n\nIn this notebook, we present the implementation of a **Transformer model** in PyTorch for time-series classification tasks. \n\n### Why Use a Transformer?\nTransformers are powerful models that excel at capturing long-range dependencies in sequential data. They use **multi-head self-attention** to focus on important parts of the sequence and are particularly effective when working with structured time-series data like credit card statements.\n\n### Data Format Requirements\nUsing a Transformer requires **3D input data**, but Kaggle datasets are often provided in **2D CSV format**. To preprocess the data for a Transformer model, we need to reshape it into the required format:\n\n- **Expected Shape**: (number_of_customers, 13, 188)\n\nWhere:\n- **`number_of_customers`**: Batch size (number of customers in each batch).\n- **`13`**: Sequence length (each customer has 13 credit card statements).\n- **`188`**: Feature length (number of features for each credit card statement).\n\nEach customer is represented as a **time series** with 13 credit card statements, making it suitable for sequence modeling.\n\n---\n\n### Links to Related Resources\n#### Kaggle Starter Notebooks\n- [GRU Starter Notebook (TensorFlow)](https://www.kaggle.com/code/cdeotte/tensorflow-gru-starter-0-790)\n- [TensorFlow Transformer Implementation - Score 0.790](https://www.kaggle.com/code/cdeotte/tensorflow-transformer-0-790)\n\n#### Exploratory Data Analysis\n- [Time-Series EDA by cdeotte](https://www.kaggle.com/cdeotte/time-series-eda)\n\n#### Kaggle Competition Discussions\n- [Discussion: Improving AMEX Competition Models](https://www.kaggle.com/competitions/amex-default-prediction/discussion/327828)\n- [Discussion: How to Train Transformers for AMEX](https://www.kaggle.com/competitions/amex-default-prediction/discussion/328054)\n- [Discussion: Sequence Modeling in the AMEX Competition](https://www.kaggle.com/competitions/amex-default-prediction/discussion/327761)","metadata":{}},{"cell_type":"code","source":"import os\nimport math\nimport numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.utils.class_weight import compute_class_weight\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport random\n\n\nHYPERPARAMETER_OPT = True\n\ndef amex_metric_mod(y_true, y_pred):\n    \"\"\"\n    Computes a custom competition metric for American Express credit default prediction.\n    This metric is used as an industry evaluation metric that takes into account business considerations.\n    It is the average of two components:\n    1. Top 4%: Measures the performance of defaulter capture in the top 4% predictions.\n    2. Gini coef: Measures the quality of ranking in comparison to a random baseline.\n    In summary: \n        - Metric score = 0.5 * (Normalized Gini + Top 4% Metric).\n\n    References:\n        https://www.kaggle.com/kyakovlev\n        https://www.kaggle.com/competitions/amex-default-prediction/discussion/327534\n    \"\"\"\n    labels = np.transpose(np.array([y_true, y_pred]))\n    labels = labels[labels[:, 1].argsort()[::-1]]\n    weights = np.where(labels[:, 0] == 0, 20, 1)\n    cut_vals = labels[np.cumsum(weights) <= int(0.04 * np.sum(weights))]\n    top_four = np.sum(cut_vals[:, 0]) / np.sum(labels[:, 0])\n\n    gini = [0, 0]\n    for i in [1, 0]:\n        labels = np.transpose(np.array([y_true, y_pred]))\n        labels = labels[labels[:, i].argsort()[::-1]]\n        weight = np.where(labels[:, 0] == 0, 20, 1)\n        weight_random = np.cumsum(weight / np.sum(weight))\n        total_pos = np.sum(labels[:, 0] * weight)\n        cum_pos_found = np.cumsum(labels[:, 0] * weight)\n        lorentz = cum_pos_found / total_pos\n        gini[i] = np.sum((lorentz - weight_random) * weight)\n\n    return 0.5 * (gini[1] / gini[0] + top_four)\n\n\nclass TransformerBlock(nn.Module):\n    \"\"\"\n    Module Class that defines a Transformer block.\n    This module implements a transformer block as defined in \"Attention Is All You Need\" paper (with some differences).\n    It is composed of :\n        - Multi-head self-attention mechanism (To capture relationships between tokens)\n        - Feed-forward network (Capturing complex patterns with feature transformation and non-linearity)\n\n    We also add scaling on the residual path so its contribution is appropriately weighted and learnable.\n\tWe also add Dropout to regularize the model to mitigated the overfitting issue and improve generalization.\n        \n    Enhanced Transformer Block with:\n    - Attention dropout\n    - Scaled residual connections\n    - Pre-Normalization\n    \"\"\"\n\n    def __init__(self, embed_dim, num_heads, ff_dim, dropout=0.1, attention_dropout=0.1):\n        super(TransformerBlock, self).__init__()\n        \n        # Multi-head self-attention\n        self.multi_head_attention = nn.MultiheadAttention(embed_dim, num_heads, batch_first=True)\n        self.attention_dropout = nn.Dropout(attention_dropout)  # Dropout for attention\n        \n        # Feed-forward network\n        self.ffn = nn.Sequential(\n            nn.Linear(embed_dim, ff_dim),\n            nn.GELU(),  # Non-linearity\n            nn.Dropout(dropout),\n            nn.Linear(ff_dim, embed_dim)\n        )\n\n        # Layer normalization (Pre-Normalization)\n        self.l_norm1 = nn.LayerNorm(embed_dim)\n        self.l_norm2 = nn.LayerNorm(embed_dim)\n\n        # Dropout for residual connections\n        self.residual_dropout = nn.Dropout(dropout)\n\n        # Learnable scaling factor for residual connections\n        self.residual_scaling1 = nn.Parameter(torch.ones(1))\n        self.residual_scaling2 = nn.Parameter(torch.ones(1))\n\n    def forward(self, x):\n        \"\"\"\n        Forward pass for the enhanced Transformer block. \n   \n        Our Input is first pre-normalized for stability and we pass it through Multi-Head Attention.\n        Plus we apply attention dropout to regularize, and a residual connection is used (with scaling) to preserve input features.\n\n        Then our  result is normalized again with LayerNorm2 and passed through a Feed-Forward Network with GELU \n        activation for non-linearity and dropout for regularization.\n\n        Finnaly we add a scaled residual connection to the feed-forward output back to the input.\n    \n        \"\"\"\n        # Pre-Normalization + Multi-Head Attention\n        norm_x = self.l_norm1(x)\n        attn_output, _ = self.multi_head_attention(norm_x, norm_x, norm_x)\n        attn_output = self.attention_dropout(attn_output)\n        \n        # Residual connection with scaling\n        x = x + self.residual_scaling1 * self.residual_dropout(attn_output)\n\n        # We apply pre-normalization and  Feed-Forward Network (GeLU)\n        norm_x = self.l_norm2(x)\n        ffn_output = self.ffn(norm_x)\n        \n        # Residual connection with scaling\n        x = x + self.residual_scaling2 * self.residual_dropout(ffn_output)\n\n        return x\n        \nclass Classifier(nn.Module):\n    \"\"\"\n    We define a classical classifier which consists of 3 fully connected layers with:\n        - ReLU activations (Non linearity) \n        - Dropout (mitigate overfitting)\n        - Sigmoid activation for binary classification (value between 0 and 1)\n    \"\"\"\n    def __init__(self, embed_dim, dropout):\n        super(Classifier, self).__init__()\n        self.fc1 = nn.Linear(embed_dim, 128)\n        self.fc2 = nn.Linear(128, 64)\n        self.fc3 = nn.Linear(64, 1)\n        # Dropout (mitigate overfitting)\n        self.dropout = nn.Dropout(dropout)\n        # Add Non linearity\n        self.relu = nn.ReLU()\n        # Sigmoid for binary classification\n        self.sigmoid = nn.Sigmoid()\n\n    def forward(self, x):\n        x = self.relu(self.fc1(x))\n        x = self.dropout(x)\n        x = self.relu(self.fc2(x))\n        x = self.dropout(x)\n        x = self.sigmoid(self.fc3(x))\n        return x\n\nclass EnhancedTransformerModel(nn.Module):\n    \"\"\"\n    A module implementing a transformer model for processing sequential (time series) and categorical data.\n    The module includes feature embedding for categorical data, positional encoding to capture time dependencies, \n    and a classification block to output the probability of default (sigmoid).\n\n    \"\"\"\n    def __init__(self, config):\n        super().__init__()\n        # We assign hyperparameters with value from a config dict (eg Dim of embedding space, Num of attn heads, ... )\n        embed_dim = config[\"embed_dim\"]\n        input_dim = config[\"input_dim\"]\n        num_heads = config[\"num_heads\"]\n        ff_dim = config[\"ff_dim\"]\n        num_blocks = config[\"num_blocks\"]\n        dropout = config[\"dropout\"]\n        activation = config.get(\"activation\", \"gelu\")\n        self.layerdrop_prob = config.get(\"layerdrop_prob\", 0.1)  # Probability of dropping a layer\n\n\n        # We implement embedding layers for 11 categorical features.\n        # nn.Embedding(10, 4): \n        #         - 10 the number of distinct indices for each categoriy.\n        #         - 4 is the size of the dense vector used to represent the value for example [0.12, -0.34, 0.56, 0.78].\n        \n        self.embedding_layers = nn.ModuleList([nn.Embedding(10, 4) for _ in range(11)])\n        self.dense = nn.Linear(input_dim, embed_dim)\n        # We add a trainable positional encoding to embeddings to provide sequence order (used in BERT)\n        self.positional_encoding = nn.Parameter(torch.zeros(1, 512, embed_dim))\n\n        # Create a seq of Transformer blocks\n        tx_list = [TransformerBlock(embed_dim, num_heads, ff_dim, dropout) for _ in range(num_blocks)]\n            \n        # Register the created blocks as a ModuleList\n        self.transformer_blocks = nn.ModuleList(tx_list)\n        \n        # We instantiate a classical classifier\n        self.classifier = Classifier(embed_dim, dropout)\n    \n    def forward(self, x):\n        \"\"\"\n        Implementation of the forward Method for the Transfomer model.\n        \n        \n        Execution Flow Diagram:\n        \n            Input (x)\n               │\n               ▼\n        Categorical Features Embedding Layers (11 embeddings) ──────┐\n               │                                                    │\n        Numerical Features (x[:, :, 11:]) ──────────────────────────┘\n               │\n               ▼\n        Concatenation (categorical embeddings + features)\n               │\n               ▼\n            Dense Layer (to adapt the size of input to Tx Block input)\n               │\n               ▼\n        Add Positional Encoding (capture sequence patterns)\n               │\n               ▼\n        Transformer Blocks\n               │\n               ▼\n        Select Last Time Step Representation as a summary (x[:, -1, :])\n               │\n               ▼\n        Classification block (Final classifier to get the likelihood of default)\n               │\n               ▼\n              Output\n        \n        \"\"\"\n        # We create 11 embeddings corresponding to the 11 categorical features present in the dataset\n        cat_embeddings = [self.embedding_layers[i](x[:, :, i].long()) for i in range(11)]\n\n        # Extract numerical features\n        x_num = x[:, :, 11:]\n        # Concatenate the numerical features with categorical features embeddings\n        x_num_cat = torch.cat( [x_num] + cat_embeddings, dim=-1)\n        \n        x = self.dense(x_num_cat)\n        # Get the sequence length (temporal data)\n        seq_length = x.size(1)\n        # Slice the positional encoding array to get the right dimensions\n        positional_encoding_slice = self.positional_encoding[:, :seq_length, :]\n        # Add the positional encoding to the concatenated tensor (numerical features & categorical features embeddings)\n        x = x + positional_encoding_slice\n        \n        # Apply transfomer blocks on inputs with residuals        \n        for tx_block in self.transformer_blocks:\n            x = tx_block(x) + x\n        \n\n        # Extract the features from the last time step, which is taken as a summary of the overall sequence \n        # (usual when you want to take into account all the sequence)\n        # It is possible because of the nature of Tx with self-attention and the multiple successive blocks\n        last_timestep = x[:, -1, :]\n        \n        output = self.classifier(last_timestep)\n            \n        return output\n\nclass TimeSeriesDataset(Dataset):\n    \"\"\"\n    An implementation of Pytorch dataset for loading and feeding the data to the target model.\n    The dataloader concatenates the split data and target files into their respective unified numpy arrays (data and label).\n    \n    \"\"\"\n    def __init__(self, data_files, target_files):\n        self.data = np.concatenate([np.load(file) for file in data_files], axis=0)\n        self.targets = pd.concat([pd.read_parquet(file) for file in target_files]).target.values\n    \n    def __len__(self):\n        return len(self.targets)\n    \n    def __getitem__(self, idx):\n        return torch.tensor(self.data[idx], dtype=torch.float32), torch.tensor(self.targets[idx], dtype=torch.float32)\n\ndef weighted_bce_loss(output, target, class_weights):\n    \"\"\"\n    An implementation of weighted version of the Binary Cross-Entropy (BCE) loss. \n    It adjusts BCE for class imbalance in binary classification by applying specific weights to mitigate the difference. \n    For info : BCE is a classical loss function used in binary classification tasks as it measures \n    the difference between predicted probabilities (output of the network sigmoid function) and actual labels (target).\n    \n    The loss for a single data point is calculated as:\n        W_BCE Loss = - (w_0 * (1 - y) * log(1 - y_hat) + w_1 * y * log(y_hat))\n    Where:\n        - y: The actual label (target in dataset) (takes values of 0 or 1).\n        - y_hat: The predicted probability of the defaulting (value between 0 and 1).\n        - w_*: The weight for class 0/1.\n    \"\"\"\n    weights = torch.where(target == 0, class_weights[0], class_weights[1])\n    return nn.BCELoss(weight=weights)(output, target)\n\ndef train_one_epoch(model, loader, optimizer, class_weights):\n    \"\"\"\n    Implementation of a training function for a single epoch (will be called in the training loop).\n    it loads the data does a forwards pass, computes the loss and uses backpropagation to compute new weights. \n    It is using the Weighted BCE loss (mentionned earlier regarding class imbalance).\n    \"\"\"\n    model.train()\n    total_loss = 0\n    for data, target in loader:\n        data, target = data.to(device), target.to(device)\n        optimizer.zero_grad()\n        output = model(data).squeeze()\n        loss = weighted_bce_loss(output, target, class_weights)\n        loss.backward()\n        optimizer.step()\n        total_loss += loss.item()\n    return total_loss / len(loader)\n\ndef validate(model, loader, class_weights):\n    \"\"\"\n    Implementation of a Validation function.\n    Puts the model into evaluation mode.\n    it loads the data does a forwards pass, computes the loss and uses backpropagation to compute new weights. \n    It is using the Weighted BCE loss (mentionned earlier regarding class imbalance).\n    outputs the predictions and labels to be used for metric computations.\n    \"\"\"\n    model.eval()\n    total_loss, predictions, actuals = 0, [], []\n    with torch.no_grad():\n        for data, target in loader:\n            data, target = data.to(device), target.to(device)\n            output = model(data).squeeze()\n            loss = weighted_bce_loss(output, target, class_weights)\n            total_loss += loss.item()\n            predictions.append(output.cpu().numpy())\n            actuals.append(target.cpu().numpy())\n    return total_loss / len(loader), np.concatenate(predictions), np.concatenate(actuals)\n\ndef train_and_evaluate(config):\n    \"\"\"\n    Trains and evaluates our default prediction model (Main training loop).\n\n    The function:\n        1. Creates training and validation data loaders.\n        2. Calcultes the class weights to be used in WBCE loss computation.\n        3. Trains the model and monitoring validation metrics.\n        4. Implements an early stopping mechanism to mitigate the risks of overfitting.\n        5. Saves the best performing model according to the evaluation metric.\n    \"\"\"\n\n    # Creates data loader to feed the model training/evaluation loop\n    train_loader, valid_loader = prepare_data(config[\"batch_size\"])\n\n    # Computes the class weights for WBCE to mitigate the bias due to class imbalance\n    targets = pd.concat([pd.read_parquet(f) for f in train_target_files]).target.values\n    class_weights = compute_class_weight('balanced', classes=np.unique(targets), y=targets)\n    class_weights = torch.tensor(class_weights, dtype=torch.float32).to(device)\n\n    # Instantiates the model\n    model = EnhancedTransformerModel(config).to(device)\n    # Creeate an AdamW optmizer (known for providing training stability)\n    optimizer = optim.AdamW(model.parameters(), lr=config[\"learning_rate\"], weight_decay=config[\"weight_decay\"])\n    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=3, verbose=True)\n\n    best_valid_metric = 0\n    early_stop_counter = 0\n\n    for epoch in range(config[\"num_epochs\"]):\n        train_loss = train_one_epoch(model, train_loader, optimizer, class_weights)\n        valid_loss, predictions, actuals = validate(model, valid_loader, class_weights)\n        valid_metric = amex_metric_mod(actuals, predictions)\n\n        if valid_metric > best_valid_metric:\n            best_valid_metric = valid_metric\n            torch.save(model.state_dict(), config[\"model_save_path\"])\n            early_stop_counter = 0\n            print(f\"New best model saved at epoch {epoch+1} with AMEX Metric: {valid_metric:.4f}\")\n        else:\n            early_stop_counter += 1\n\n        print(f\"Epoch {epoch+1}, Train Loss: {train_loss:.4f}, Valid Loss: {valid_loss:.4f}, AMEX Metric: {valid_metric:.4f}\")\n        scheduler.step(valid_loss)\n\n        if early_stop_counter >= config[\"patience\"]:\n            print(f\"Early stopping at epoch {epoch+1}\")\n            break\n\n    return model, predictions, actuals\n\ndef prepare_data(batch_size):\n    train_loader = DataLoader(TimeSeriesDataset(train_data_files, train_target_files), batch_size=batch_size, shuffle=True)\n    valid_loader = DataLoader(TimeSeriesDataset(valid_data_files, valid_target_files), batch_size=batch_size)\n    return train_loader, valid_loader\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-08T19:58:48.199666Z","iopub.execute_input":"2024-12-08T19:58:48.199953Z","iopub.status.idle":"2024-12-08T19:58:52.317086Z","shell.execute_reply.started":"2024-12-08T19:58:48.199927Z","shell.execute_reply":"2024-12-08T19:58:52.316207Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(\"Running on:\", device)\n\ntrain_data_files = [f'/kaggle/input/feature-engineering-plus-plus/processed_data/data_{i}.npy' for i in range(1, 9)]\ntrain_target_files = [f'/kaggle/input/feature-engineering-plus-plus/processed_data/targets_{i}.pqt' for i in range(1, 9)]\nvalid_data_files = [f'/kaggle/input/feature-engineering-plus-plus/processed_data/data_{i}.npy' for i in range(9, 11)]\nvalid_target_files = [f'/kaggle/input/feature-engineering-plus-plus/processed_data/targets_{i}.pqt' for i in range(9, 11)]\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-08T19:58:52.318747Z","iopub.execute_input":"2024-12-08T19:58:52.319212Z","iopub.status.idle":"2024-12-08T19:58:52.386780Z","shell.execute_reply.started":"2024-12-08T19:58:52.319171Z","shell.execute_reply":"2024-12-08T19:58:52.385955Z"}},"outputs":[{"name":"stdout","text":"Running on: cuda\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"\nif HYPERPARAMETER_OPT:\n    import optuna\n    from optuna.trial import TrialState\n    \n    def objective(trial):\n        \"\"\"\n        A function defining the objective function for conducting hyperparameter optimization\n        for a our default detection model using Optuna.\n        The performance metric to be optimized (maximized ) is the AMEX industry metric (top 4% and Gini coef average).\n        \"\"\"\n        config = {\n            \"embed_dim\": trial.suggest_categorical(\"embed_dim\", [64, 128, 256]),\n            \"input_dim\": 221,\n            \"num_heads\": trial.suggest_categorical(\"num_heads\", [4, 8, 16]),\n            \"ff_dim\": trial.suggest_categorical(\"ff_dim\", [256, 512, 1024]),\n            \"num_blocks\": trial.suggest_int(\"num_blocks\", 2, 6),\n            \"dropout\": trial.suggest_float(\"dropout\", 0.1, 0.5),\n            \"batch_size\": trial.suggest_categorical(\"batch_size\", [128, 256, 512]),\n            \"learning_rate\": trial.suggest_float(\"learning_rate\", 1e-5, 1e-2, log=True),\n            \"weight_decay\": trial.suggest_float(\"weight_decay\", 1e-6, 1e-3, log=True),\n            \"patience\": 5,\n            \"num_epochs\": 10,\n            \"model_save_path\": f\"/kaggle/working/best_transformer_model_trial_{trial.number}.pth\",\n            \"threshold\": 0.5\n        }\n    \n        model, predictions, actuals = train_and_evaluate(config)\n        amex_score = amex_metric_mod(actuals, predictions)\n        return amex_score\n    \n    study = optuna.create_study(direction=\"maximize\")\n    study.optimize(objective, n_trials=10)\n    \n    print(\"Number of completed trials:\", len(study.trials))\n    print(\"Best trial:\")\n    best_trial = study.best_trial\n    print(f\"  Value: {best_trial.value}\")\n    print(\"  Params:\")\n    for key, value in best_trial.params.items():\n        print(f\"    {key}: {value}\")\n    \n    best_config = {\n        \"embed_dim\": best_trial.params[\"embed_dim\"],\n        \"input_dim\": 221,\n        \"num_heads\": best_trial.params[\"num_heads\"],\n        \"ff_dim\": best_trial.params[\"ff_dim\"],\n        \"num_blocks\": best_trial.params[\"num_blocks\"],\n        \"dropout\": best_trial.params[\"dropout\"],\n        \"batch_size\": best_trial.params[\"batch_size\"],\n        \"learning_rate\": best_trial.params[\"learning_rate\"],\n        \"weight_decay\": best_trial.params[\"weight_decay\"],\n        \"patience\": 5,\n        \"num_epochs\": 10,\n        \"model_save_path\": \"/kaggle/working/best_transformer_model_final.pth\",\n        \"threshold\": 0.5\n    }\n    \n    model, predictions, actuals = train_and_evaluate(best_config)\n    \n    platt_scaler = LogisticRegression()\n    platt_scaler.fit(predictions.reshape(-1, 1), actuals)\n    calibrated_probs = platt_scaler.predict_proba(predictions.reshape(-1, 1))[:, 1]\n    \n    binary_preds = (calibrated_probs >= best_config[\"threshold\"]).astype(int)\n    cm = confusion_matrix(actuals, binary_preds)\n    print(f\"Final AMEX Metric: {amex_metric_mod(actuals, calibrated_probs):.4f}\")\n    print(f\"Precision: {precision_score(actuals, binary_preds):.4f}\")\n    print(f\"Recall: {recall_score(actuals, binary_preds):.4f}\")\n    print(f\"F1-Score: {f1_score(actuals, binary_preds):.4f}\")\n    \n    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['No Default', 'Default'], yticklabels=['No Default', 'Default'])\n    plt.xlabel('Predicted')\n    plt.ylabel('Actual')\n    plt.title('Confusion Matrix')\n    plt.show()\n\n\n    # Save the Best Hyperparameters dictionary to an .npy file\n    np.save('/kaggle/working/best_config.npy', best_config)\n    \n    print(\" Best Hyperparameters dictionary saved to 'best_config.npy'\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-08T17:45:20.682772Z","iopub.execute_input":"2024-12-08T17:45:20.683062Z","iopub.status.idle":"2024-12-08T18:52:35.150755Z","shell.execute_reply.started":"2024-12-08T17:45:20.683034Z","shell.execute_reply":"2024-12-08T18:52:35.149915Z"}},"outputs":[{"name":"stderr","text":"[I 2024-12-08 17:45:20,786] A new study created in memory with name: no-name-53910b87-b3b9-47bc-b84a-4f8b81148237\n/opt/conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"New best model saved at epoch 1 with AMEX Metric: 0.7758\nEpoch 1, Train Loss: 0.2833, Valid Loss: 0.2716, AMEX Metric: 0.7758\nNew best model saved at epoch 2 with AMEX Metric: 0.7780\nEpoch 2, Train Loss: 0.2703, Valid Loss: 0.2597, AMEX Metric: 0.7780\nNew best model saved at epoch 3 with AMEX Metric: 0.7797\nEpoch 3, Train Loss: 0.2698, Valid Loss: 0.2602, AMEX Metric: 0.7797\nNew best model saved at epoch 4 with AMEX Metric: 0.7825\nEpoch 4, Train Loss: 0.2689, Valid Loss: 0.2568, AMEX Metric: 0.7825\nEpoch 5, Train Loss: 0.2649, Valid Loss: 0.2641, AMEX Metric: 0.7806\nEpoch 6, Train Loss: 0.2641, Valid Loss: 0.2603, AMEX Metric: 0.7817\nEpoch 7, Train Loss: 0.2646, Valid Loss: 0.2573, AMEX Metric: 0.7806\nNew best model saved at epoch 8 with AMEX Metric: 0.7838\nEpoch 8, Train Loss: 0.2627, Valid Loss: 0.2608, AMEX Metric: 0.7838\nNew best model saved at epoch 9 with AMEX Metric: 0.7850\nEpoch 9, Train Loss: 0.2581, Valid Loss: 0.2569, AMEX Metric: 0.7850\n","output_type":"stream"},{"name":"stderr","text":"[I 2024-12-08 17:50:34,981] Trial 0 finished with value: 0.787099990896981 and parameters: {'embed_dim': 64, 'num_heads': 4, 'ff_dim': 256, 'num_blocks': 3, 'dropout': 0.14693291541681255, 'batch_size': 256, 'learning_rate': 0.003337347611274545, 'weight_decay': 0.0002334106590308237}. Best is trial 0 with value: 0.787099990896981.\n","output_type":"stream"},{"name":"stdout","text":"New best model saved at epoch 10 with AMEX Metric: 0.7871\nEpoch 10, Train Loss: 0.2556, Valid Loss: 0.2574, AMEX Metric: 0.7871\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"New best model saved at epoch 1 with AMEX Metric: 0.7748\nEpoch 1, Train Loss: 0.2837, Valid Loss: 0.2609, AMEX Metric: 0.7748\nNew best model saved at epoch 2 with AMEX Metric: 0.7799\nEpoch 2, Train Loss: 0.2669, Valid Loss: 0.2594, AMEX Metric: 0.7799\nNew best model saved at epoch 3 with AMEX Metric: 0.7826\nEpoch 3, Train Loss: 0.2619, Valid Loss: 0.2559, AMEX Metric: 0.7826\nEpoch 4, Train Loss: 0.2598, Valid Loss: 0.2585, AMEX Metric: 0.7817\nNew best model saved at epoch 5 with AMEX Metric: 0.7858\nEpoch 5, Train Loss: 0.2576, Valid Loss: 0.2533, AMEX Metric: 0.7858\nEpoch 6, Train Loss: 0.2555, Valid Loss: 0.2537, AMEX Metric: 0.7840\nNew best model saved at epoch 7 with AMEX Metric: 0.7868\nEpoch 7, Train Loss: 0.2545, Valid Loss: 0.2515, AMEX Metric: 0.7868\nEpoch 8, Train Loss: 0.2527, Valid Loss: 0.2520, AMEX Metric: 0.7861\nEpoch 9, Train Loss: 0.2507, Valid Loss: 0.2533, AMEX Metric: 0.7857\n","output_type":"stream"},{"name":"stderr","text":"[I 2024-12-08 17:56:37,418] Trial 1 finished with value: 0.7860872692397921 and parameters: {'embed_dim': 128, 'num_heads': 4, 'ff_dim': 1024, 'num_blocks': 4, 'dropout': 0.12276482939784757, 'batch_size': 256, 'learning_rate': 0.0006636750338246849, 'weight_decay': 0.00023767399368120054}. Best is trial 0 with value: 0.787099990896981.\n","output_type":"stream"},{"name":"stdout","text":"Epoch 10, Train Loss: 0.2493, Valid Loss: 0.2564, AMEX Metric: 0.7861\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"New best model saved at epoch 1 with AMEX Metric: 0.7298\nEpoch 1, Train Loss: 0.4090, Valid Loss: 0.2945, AMEX Metric: 0.7298\nNew best model saved at epoch 2 with AMEX Metric: 0.7495\nEpoch 2, Train Loss: 0.3090, Valid Loss: 0.2794, AMEX Metric: 0.7495\nNew best model saved at epoch 3 with AMEX Metric: 0.7572\nEpoch 3, Train Loss: 0.2958, Valid Loss: 0.2720, AMEX Metric: 0.7572\nNew best model saved at epoch 4 with AMEX Metric: 0.7624\nEpoch 4, Train Loss: 0.2880, Valid Loss: 0.2679, AMEX Metric: 0.7624\nNew best model saved at epoch 5 with AMEX Metric: 0.7649\nEpoch 5, Train Loss: 0.2831, Valid Loss: 0.2664, AMEX Metric: 0.7649\nNew best model saved at epoch 6 with AMEX Metric: 0.7692\nEpoch 6, Train Loss: 0.2814, Valid Loss: 0.2641, AMEX Metric: 0.7692\nNew best model saved at epoch 7 with AMEX Metric: 0.7707\nEpoch 7, Train Loss: 0.2786, Valid Loss: 0.2631, AMEX Metric: 0.7707\nNew best model saved at epoch 8 with AMEX Metric: 0.7722\nEpoch 8, Train Loss: 0.2766, Valid Loss: 0.2622, AMEX Metric: 0.7722\nNew best model saved at epoch 9 with AMEX Metric: 0.7729\nEpoch 9, Train Loss: 0.2746, Valid Loss: 0.2610, AMEX Metric: 0.7729\n","output_type":"stream"},{"name":"stderr","text":"[I 2024-12-08 18:01:34,513] Trial 2 finished with value: 0.7752853866297189 and parameters: {'embed_dim': 256, 'num_heads': 8, 'ff_dim': 256, 'num_blocks': 3, 'dropout': 0.35857482616165115, 'batch_size': 512, 'learning_rate': 1.0429707304665036e-05, 'weight_decay': 1.0800078677353132e-05}. Best is trial 0 with value: 0.787099990896981.\n","output_type":"stream"},{"name":"stdout","text":"New best model saved at epoch 10 with AMEX Metric: 0.7753\nEpoch 10, Train Loss: 0.2719, Valid Loss: 0.2605, AMEX Metric: 0.7753\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"New best model saved at epoch 1 with AMEX Metric: 0.7716\nEpoch 1, Train Loss: 0.3068, Valid Loss: 0.2643, AMEX Metric: 0.7716\nNew best model saved at epoch 2 with AMEX Metric: 0.7776\nEpoch 2, Train Loss: 0.2741, Valid Loss: 0.2671, AMEX Metric: 0.7776\nNew best model saved at epoch 3 with AMEX Metric: 0.7797\nEpoch 3, Train Loss: 0.2685, Valid Loss: 0.2620, AMEX Metric: 0.7797\nNew best model saved at epoch 4 with AMEX Metric: 0.7820\nEpoch 4, Train Loss: 0.2647, Valid Loss: 0.2557, AMEX Metric: 0.7820\nNew best model saved at epoch 5 with AMEX Metric: 0.7838\nEpoch 5, Train Loss: 0.2634, Valid Loss: 0.2643, AMEX Metric: 0.7838\nEpoch 6, Train Loss: 0.2623, Valid Loss: 0.2648, AMEX Metric: 0.7828\nNew best model saved at epoch 7 with AMEX Metric: 0.7855\nEpoch 7, Train Loss: 0.2630, Valid Loss: 0.2669, AMEX Metric: 0.7855\nEpoch 8, Train Loss: 0.2610, Valid Loss: 0.2647, AMEX Metric: 0.7853\nNew best model saved at epoch 9 with AMEX Metric: 0.7872\nEpoch 9, Train Loss: 0.2536, Valid Loss: 0.2575, AMEX Metric: 0.7872\n","output_type":"stream"},{"name":"stderr","text":"[I 2024-12-08 18:07:06,236] Trial 3 finished with value: 0.7875796440695455 and parameters: {'embed_dim': 128, 'num_heads': 8, 'ff_dim': 512, 'num_blocks': 5, 'dropout': 0.2548459840071933, 'batch_size': 512, 'learning_rate': 0.0014923296145369208, 'weight_decay': 2.7711839477218094e-06}. Best is trial 3 with value: 0.7875796440695455.\n","output_type":"stream"},{"name":"stdout","text":"New best model saved at epoch 10 with AMEX Metric: 0.7876\nEpoch 10, Train Loss: 0.2510, Valid Loss: 0.2520, AMEX Metric: 0.7876\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"New best model saved at epoch 1 with AMEX Metric: 0.7660\nEpoch 1, Train Loss: 0.3194, Valid Loss: 0.2731, AMEX Metric: 0.7660\nNew best model saved at epoch 2 with AMEX Metric: 0.7753\nEpoch 2, Train Loss: 0.2840, Valid Loss: 0.2661, AMEX Metric: 0.7753\nNew best model saved at epoch 3 with AMEX Metric: 0.7787\nEpoch 3, Train Loss: 0.2753, Valid Loss: 0.2645, AMEX Metric: 0.7787\nNew best model saved at epoch 4 with AMEX Metric: 0.7798\nEpoch 4, Train Loss: 0.2680, Valid Loss: 0.2579, AMEX Metric: 0.7798\nNew best model saved at epoch 5 with AMEX Metric: 0.7799\nEpoch 5, Train Loss: 0.2661, Valid Loss: 0.2581, AMEX Metric: 0.7799\nNew best model saved at epoch 6 with AMEX Metric: 0.7831\nEpoch 6, Train Loss: 0.2619, Valid Loss: 0.2541, AMEX Metric: 0.7831\nNew best model saved at epoch 7 with AMEX Metric: 0.7832\nEpoch 7, Train Loss: 0.2599, Valid Loss: 0.2555, AMEX Metric: 0.7832\nNew best model saved at epoch 8 with AMEX Metric: 0.7834\nEpoch 8, Train Loss: 0.2581, Valid Loss: 0.2573, AMEX Metric: 0.7834\nEpoch 9, Train Loss: 0.2556, Valid Loss: 0.2549, AMEX Metric: 0.7832\n","output_type":"stream"},{"name":"stderr","text":"[I 2024-12-08 18:15:41,757] Trial 4 finished with value: 0.7822144939672613 and parameters: {'embed_dim': 256, 'num_heads': 16, 'ff_dim': 256, 'num_blocks': 5, 'dropout': 0.17717038468801644, 'batch_size': 256, 'learning_rate': 4.297159854979993e-05, 'weight_decay': 7.474224844064137e-06}. Best is trial 3 with value: 0.7875796440695455.\n","output_type":"stream"},{"name":"stdout","text":"Epoch 10, Train Loss: 0.2546, Valid Loss: 0.2600, AMEX Metric: 0.7822\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"New best model saved at epoch 1 with AMEX Metric: 0.7669\nEpoch 1, Train Loss: 0.3285, Valid Loss: 0.3094, AMEX Metric: 0.7669\nEpoch 2, Train Loss: 0.3323, Valid Loss: 0.2934, AMEX Metric: 0.7642\nEpoch 3, Train Loss: 0.3389, Valid Loss: 0.2918, AMEX Metric: 0.7616\nEpoch 4, Train Loss: 0.3574, Valid Loss: 0.3074, AMEX Metric: 0.7626\nEpoch 5, Train Loss: 0.3548, Valid Loss: 0.2839, AMEX Metric: 0.7612\n","output_type":"stream"},{"name":"stderr","text":"[I 2024-12-08 18:20:56,605] Trial 5 finished with value: 0.5771822540902933 and parameters: {'embed_dim': 256, 'num_heads': 4, 'ff_dim': 512, 'num_blocks': 4, 'dropout': 0.39142975878631936, 'batch_size': 128, 'learning_rate': 0.0034725624290396447, 'weight_decay': 2.2700287415134136e-06}. Best is trial 3 with value: 0.7875796440695455.\n","output_type":"stream"},{"name":"stdout","text":"Epoch 6, Train Loss: 0.3807, Valid Loss: 0.3187, AMEX Metric: 0.5772\nEarly stopping at epoch 6\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"New best model saved at epoch 1 with AMEX Metric: 0.7724\nEpoch 1, Train Loss: 0.2981, Valid Loss: 0.2637, AMEX Metric: 0.7724\nNew best model saved at epoch 2 with AMEX Metric: 0.7773\nEpoch 2, Train Loss: 0.2694, Valid Loss: 0.2668, AMEX Metric: 0.7773\nNew best model saved at epoch 3 with AMEX Metric: 0.7808\nEpoch 3, Train Loss: 0.2623, Valid Loss: 0.2550, AMEX Metric: 0.7808\nNew best model saved at epoch 4 with AMEX Metric: 0.7841\nEpoch 4, Train Loss: 0.2605, Valid Loss: 0.2545, AMEX Metric: 0.7841\nEpoch 5, Train Loss: 0.2575, Valid Loss: 0.2653, AMEX Metric: 0.7823\nNew best model saved at epoch 6 with AMEX Metric: 0.7852\nEpoch 6, Train Loss: 0.2555, Valid Loss: 0.2541, AMEX Metric: 0.7852\nNew best model saved at epoch 7 with AMEX Metric: 0.7858\nEpoch 7, Train Loss: 0.2533, Valid Loss: 0.2584, AMEX Metric: 0.7858\nNew best model saved at epoch 8 with AMEX Metric: 0.7860\nEpoch 8, Train Loss: 0.2516, Valid Loss: 0.2538, AMEX Metric: 0.7860\nNew best model saved at epoch 9 with AMEX Metric: 0.7875\nEpoch 9, Train Loss: 0.2490, Valid Loss: 0.2560, AMEX Metric: 0.7875\n","output_type":"stream"},{"name":"stderr","text":"[I 2024-12-08 18:27:02,486] Trial 6 finished with value: 0.7831804774886212 and parameters: {'embed_dim': 128, 'num_heads': 8, 'ff_dim': 1024, 'num_blocks': 5, 'dropout': 0.13320701591657597, 'batch_size': 512, 'learning_rate': 0.0004426336456473935, 'weight_decay': 1.8686478421632118e-05}. Best is trial 3 with value: 0.7875796440695455.\n","output_type":"stream"},{"name":"stdout","text":"Epoch 10, Train Loss: 0.2467, Valid Loss: 0.2608, AMEX Metric: 0.7832\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"New best model saved at epoch 1 with AMEX Metric: 0.7631\nEpoch 1, Train Loss: 0.3162, Valid Loss: 0.2674, AMEX Metric: 0.7631\nNew best model saved at epoch 2 with AMEX Metric: 0.7716\nEpoch 2, Train Loss: 0.2742, Valid Loss: 0.2601, AMEX Metric: 0.7716\nNew best model saved at epoch 3 with AMEX Metric: 0.7742\nEpoch 3, Train Loss: 0.2687, Valid Loss: 0.2590, AMEX Metric: 0.7742\nNew best model saved at epoch 4 with AMEX Metric: 0.7771\nEpoch 4, Train Loss: 0.2659, Valid Loss: 0.2575, AMEX Metric: 0.7771\nNew best model saved at epoch 5 with AMEX Metric: 0.7784\nEpoch 5, Train Loss: 0.2637, Valid Loss: 0.2556, AMEX Metric: 0.7784\nNew best model saved at epoch 6 with AMEX Metric: 0.7799\nEpoch 6, Train Loss: 0.2625, Valid Loss: 0.2555, AMEX Metric: 0.7799\nNew best model saved at epoch 7 with AMEX Metric: 0.7809\nEpoch 7, Train Loss: 0.2607, Valid Loss: 0.2552, AMEX Metric: 0.7809\nNew best model saved at epoch 8 with AMEX Metric: 0.7814\nEpoch 8, Train Loss: 0.2595, Valid Loss: 0.2537, AMEX Metric: 0.7814\nNew best model saved at epoch 9 with AMEX Metric: 0.7816\nEpoch 9, Train Loss: 0.2583, Valid Loss: 0.2551, AMEX Metric: 0.7816\n","output_type":"stream"},{"name":"stderr","text":"[I 2024-12-08 18:32:03,858] Trial 7 finished with value: 0.7844922375298407 and parameters: {'embed_dim': 256, 'num_heads': 8, 'ff_dim': 512, 'num_blocks': 2, 'dropout': 0.28235876919469993, 'batch_size': 256, 'learning_rate': 2.8111919565509177e-05, 'weight_decay': 9.224054099406043e-06}. Best is trial 3 with value: 0.7875796440695455.\n","output_type":"stream"},{"name":"stdout","text":"New best model saved at epoch 10 with AMEX Metric: 0.7845\nEpoch 10, Train Loss: 0.2566, Valid Loss: 0.2528, AMEX Metric: 0.7845\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"New best model saved at epoch 1 with AMEX Metric: 0.7636\nEpoch 1, Train Loss: 0.3487, Valid Loss: 0.2771, AMEX Metric: 0.7636\nNew best model saved at epoch 2 with AMEX Metric: 0.7716\nEpoch 2, Train Loss: 0.2908, Valid Loss: 0.2692, AMEX Metric: 0.7716\nNew best model saved at epoch 3 with AMEX Metric: 0.7735\nEpoch 3, Train Loss: 0.2808, Valid Loss: 0.2724, AMEX Metric: 0.7735\nNew best model saved at epoch 4 with AMEX Metric: 0.7755\nEpoch 4, Train Loss: 0.2765, Valid Loss: 0.2671, AMEX Metric: 0.7755\nNew best model saved at epoch 5 with AMEX Metric: 0.7794\nEpoch 5, Train Loss: 0.2706, Valid Loss: 0.2687, AMEX Metric: 0.7794\nNew best model saved at epoch 6 with AMEX Metric: 0.7815\nEpoch 6, Train Loss: 0.2683, Valid Loss: 0.2636, AMEX Metric: 0.7815\nNew best model saved at epoch 7 with AMEX Metric: 0.7823\nEpoch 7, Train Loss: 0.2655, Valid Loss: 0.2575, AMEX Metric: 0.7823\nEpoch 8, Train Loss: 0.2640, Valid Loss: 0.2573, AMEX Metric: 0.7814\nNew best model saved at epoch 9 with AMEX Metric: 0.7852\nEpoch 9, Train Loss: 0.2617, Valid Loss: 0.2565, AMEX Metric: 0.7852\n","output_type":"stream"},{"name":"stderr","text":"[I 2024-12-08 18:42:24,900] Trial 8 finished with value: 0.7828742991011799 and parameters: {'embed_dim': 256, 'num_heads': 16, 'ff_dim': 256, 'num_blocks': 5, 'dropout': 0.38843773819314487, 'batch_size': 128, 'learning_rate': 4.75053474914503e-05, 'weight_decay': 0.00019428408153117284}. Best is trial 3 with value: 0.7875796440695455.\n","output_type":"stream"},{"name":"stdout","text":"Epoch 10, Train Loss: 0.2605, Valid Loss: 0.2596, AMEX Metric: 0.7829\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"New best model saved at epoch 1 with AMEX Metric: 0.7713\nEpoch 1, Train Loss: 0.3017, Valid Loss: 0.2782, AMEX Metric: 0.7713\nNew best model saved at epoch 2 with AMEX Metric: 0.7768\nEpoch 2, Train Loss: 0.2787, Valid Loss: 0.2661, AMEX Metric: 0.7768\nNew best model saved at epoch 3 with AMEX Metric: 0.7810\nEpoch 3, Train Loss: 0.2758, Valid Loss: 0.2597, AMEX Metric: 0.7810\nEpoch 4, Train Loss: 0.2746, Valid Loss: 0.2625, AMEX Metric: 0.7797\nNew best model saved at epoch 5 with AMEX Metric: 0.7819\nEpoch 5, Train Loss: 0.2734, Valid Loss: 0.2762, AMEX Metric: 0.7819\nEpoch 6, Train Loss: 0.2729, Valid Loss: 0.2642, AMEX Metric: 0.7783\nNew best model saved at epoch 7 with AMEX Metric: 0.7838\nEpoch 7, Train Loss: 0.2725, Valid Loss: 0.2735, AMEX Metric: 0.7838\nNew best model saved at epoch 8 with AMEX Metric: 0.7850\nEpoch 8, Train Loss: 0.2637, Valid Loss: 0.2679, AMEX Metric: 0.7850\nNew best model saved at epoch 9 with AMEX Metric: 0.7860\nEpoch 9, Train Loss: 0.2613, Valid Loss: 0.2628, AMEX Metric: 0.7860\n","output_type":"stream"},{"name":"stderr","text":"[I 2024-12-08 18:47:03,760] Trial 9 finished with value: 0.7858892772653796 and parameters: {'embed_dim': 128, 'num_heads': 8, 'ff_dim': 256, 'num_blocks': 4, 'dropout': 0.43918877345825014, 'batch_size': 512, 'learning_rate': 0.002221182882003344, 'weight_decay': 2.618060520908976e-05}. Best is trial 3 with value: 0.7875796440695455.\n","output_type":"stream"},{"name":"stdout","text":"Epoch 10, Train Loss: 0.2604, Valid Loss: 0.2546, AMEX Metric: 0.7859\nNumber of completed trials: 10\nBest trial:\n  Value: 0.7875796440695455\n  Params:\n    embed_dim: 128\n    num_heads: 8\n    ff_dim: 512\n    num_blocks: 5\n    dropout: 0.2548459840071933\n    batch_size: 512\n    learning_rate: 0.0014923296145369208\n    weight_decay: 2.7711839477218094e-06\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"New best model saved at epoch 1 with AMEX Metric: 0.7721\nEpoch 1, Train Loss: 0.2973, Valid Loss: 0.2659, AMEX Metric: 0.7721\nNew best model saved at epoch 2 with AMEX Metric: 0.7773\nEpoch 2, Train Loss: 0.2718, Valid Loss: 0.2645, AMEX Metric: 0.7773\nNew best model saved at epoch 3 with AMEX Metric: 0.7811\nEpoch 3, Train Loss: 0.2685, Valid Loss: 0.2571, AMEX Metric: 0.7811\nNew best model saved at epoch 4 with AMEX Metric: 0.7842\nEpoch 4, Train Loss: 0.2656, Valid Loss: 0.2545, AMEX Metric: 0.7842\nEpoch 5, Train Loss: 0.2643, Valid Loss: 0.2581, AMEX Metric: 0.7810\nEpoch 6, Train Loss: 0.2632, Valid Loss: 0.2588, AMEX Metric: 0.7834\nEpoch 7, Train Loss: 0.2608, Valid Loss: 0.2623, AMEX Metric: 0.7827\nNew best model saved at epoch 8 with AMEX Metric: 0.7847\nEpoch 8, Train Loss: 0.2601, Valid Loss: 0.2643, AMEX Metric: 0.7847\nNew best model saved at epoch 9 with AMEX Metric: 0.7864\nEpoch 9, Train Loss: 0.2544, Valid Loss: 0.2604, AMEX Metric: 0.7864\nNew best model saved at epoch 10 with AMEX Metric: 0.7886\nEpoch 10, Train Loss: 0.2511, Valid Loss: 0.2573, AMEX Metric: 0.7886\nFinal AMEX Metric: 0.7886\nPrecision: 0.7921\nRecall: 0.8371\nF1-Score: 0.8140\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<Figure size 640x480 with 2 Axes>","image/png":"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"},"metadata":{}},{"name":"stdout","text":" Best Hyperparameters dictionary saved to 'best_config.npy'\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}